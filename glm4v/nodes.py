import os
import re
import torch
from transformers import AutoProcessor, Glm4vForConditionalGeneration
import comfy
import folder_paths
from PIL import Image
import numpy as np
import json
from enum import Enum

# è·å–å½“å‰ComfyUIæ ¹ç›®å½•
current_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
models_path = os.path.join(current_dir, 'models/LLM')

class GLM4VImageDescription_V2:
    DESCRIPTION = """
    GLM-4V å›¾åƒæè¿°ç”Ÿæˆå™¨
    åŠŸèƒ½ï¼šä½¿ç”¨GLM-4Væ¨¡å‹ç”Ÿæˆå›¾åƒæè¿°
    ä½¿ç”¨è¯´æ˜ï¼š**æ³¨æ„ï¼šåŸæ¨¡å‹åœ¨H20æ˜¾å¡ä¸Šæµ‹è¯•éœ€è‡³å°‘æœ‰24Gæ˜¾å­˜æ‰å¯è·‘**
    1. å°†æ¨¡å‹ä¸‹è½½æ”¾ç½®åœ¨models/LLMç›®å½•ä¸‹
    2. é€‰æ‹©æ¨¡å‹è·¯å¾„å’Œè¾“å…¥å›¾åƒ
    3. è®¾ç½®ç”Ÿæˆå‚æ•°ï¼ˆæ¸©åº¦ã€top_pç­‰ï¼‰
    4. è¾“å…¥ç”¨æˆ·è¾“å…¥ï¼ˆé»˜è®¤ä¸º"ç®€å•æè¿°è¿™å¼ å›¾ç‰‡å†…å®¹"ï¼‰
    5. ç‚¹å‡»ç”Ÿæˆè·å–æè¿°ç»“æœ
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        # æ‰«æ/models/LLMç›®å½•ä¸‹å¯ç”¨çš„æ¨¡å‹
        models = []
        if os.path.exists(models_path):
            models = [d for d in os.listdir(models_path) if os.path.isdir(os.path.join(models_path, d))]
        
        return {
            "required": {
                "å›¾åƒ": ("IMAGE",),
                "æ¨¡å‹è·¯å¾„": (models, {"default": "GLM-4.1V-9B-Thinking"} if models else {}),
                # "æ¨¡å‹è·¯å¾„": ("STRING", {"default": "", "multiline": False, "display_name": "æ¨¡å‹è·¯å¾„"}),
                "ç”¨æˆ·è¾“å…¥": ("STRING", {"multiline": True, "default": "ç®€å•æè¿°è¿™å¼ å›¾ç‰‡å†…å®¹"}),
                "æ¸©åº¦": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 1.0, "step": 0.01}),
                "top_p": ("FLOAT", {"default": 0.6, "min": 0.0, "max": 1.0, "step": 0.01}),
                "top_k": ("INT", {"default": 2, "min": 1, "max": 200, "step": 1}),
                "æœ€å¤§æ–°tokenæ•°": ("INT", {"default": 8192, "min": 512, "max": 16384, "step": 1}),
                "é‡å¤æƒ©ç½š": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 2.0, "step": 0.01}),
                "å¸è½½æ¨¡å‹": ("BOOLEAN", {"default": True}),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("æè¿°",)
    FUNCTION = "describe_image"
    CATEGORY = "ğŸš¬é¦™çƒŸçš„å·¥å…·ç®±âœ…/3ï¸âƒ£æç¤ºè¯åæ¨ğŸ“"
    
    def __init__(self):
        self.model = None
        self.processor = None
    
    def describe_image(self, å›¾åƒ, æ¨¡å‹è·¯å¾„, ç”¨æˆ·è¾“å…¥, æ¸©åº¦, top_p, top_k, æœ€å¤§æ–°tokenæ•°, é‡å¤æƒ©ç½š, å¸è½½æ¨¡å‹):
        # å¦‚æœæ¨¡å‹å·²åŠ è½½ä¸”éœ€è¦å¸è½½ï¼Œåˆ™å…ˆå¸è½½
        if å¸è½½æ¨¡å‹ and self.model is not None:
            del self.model
            del self.processor
            self.model = None
            self.processor = None
            torch.cuda.empty_cache()
        
        # å°†ComfyUIå›¾åƒå¼ é‡è½¬æ¢ä¸ºPILå›¾åƒ
        # å›¾åƒå¼ é‡æ ¼å¼ä¸º[N, H, W, C]ï¼Œå…¶ä¸­N=1, C=3 (RGB)
        image_np = å›¾åƒ[0].numpy() * 255.0
        image_np = np.clip(image_np, 0, 255).astype(np.uint8)
        pil_image = Image.fromarray(image_np)
        
        # ä¿å­˜å›¾åƒåˆ°ä¸´æ—¶æ–‡ä»¶
        temp_image_path = os.path.join(folder_paths.get_temp_directory(), "glm4v_temp_image.png")
        pil_image.save(temp_image_path)
                
        # æ„å»ºæ¶ˆæ¯
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "url": temp_image_path},
                    {"type": "text", "text": ç”¨æˆ·è¾“å…¥}
                ]
            }
        ]
        
        # å¦‚æœæ¨¡å‹æœªåŠ è½½ï¼Œåˆ™åŠ è½½æ¨¡å‹
        if self.model is None:
            full_model_path = os.path.join(models_path, æ¨¡å‹è·¯å¾„)
            self.processor = AutoProcessor.from_pretrained(full_model_path, use_fast=True)
            self.model = Glm4vForConditionalGeneration.from_pretrained(
                full_model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                attn_implementation="sdpa"
            )
        
        # å¤„ç†è¾“å…¥
        inputs = self.processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt",
            padding=True
        ).to(self.model.device)
                
        # ç”Ÿæˆè¾“å‡º
        output = self.model.generate(
            **inputs,
            max_new_tokens=æœ€å¤§æ–°tokenæ•°,
            repetition_penalty=é‡å¤æƒ©ç½š,
            do_sample=æ¸©åº¦ > 0,
            top_k=top_k,
            top_p=top_p,
            temperature=æ¸©åº¦ if æ¸©åº¦ > 0 else None,
        )
        
        # è§£ç è¾“å‡º
        raw = self.processor.decode(
            output[0][inputs["input_ids"].shape[1]:-1],
            skip_special_tokens=True
        )
        
        # å°è¯•è§£æJSONæ ¼å¼çš„è¾“å‡º
        json_data = None
        json_match = None
        if raw.startswith('{') or raw.startswith('['):
            json_data = json.loads(raw)
            json_match = re.search(r'<answer>(.*)', json.dumps(json_data), re.DOTALL)
        
        # åŒ¹é…ç­”æ¡ˆ
        original_match = re.search(r"<answer>(.*)", raw, re.DOTALL)
        match = json_match if json_match else original_match
        answer = match.group(1).strip() if match else raw.strip()
        
        print(f"[GLM4V] Prompt: {answer}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_image_path):
            os.remove(temp_image_path)
        
        # å¦‚æœéœ€è¦å¸è½½æ¨¡å‹ï¼Œåˆ™ç«‹å³å¸è½½
        if å¸è½½æ¨¡å‹:
            del self.model
            del self.processor
            self.model = None
            self.processor = None
            torch.cuda.empty_cache()
        
        return (answer,)

class MyOptions(Enum):
    flash_attention_2 = "flash_attention_2"
    sdpa = "sdpa"

class GLM4VTextToDescription:
    DESCRIPTION = """
    GLM-4V æ–‡æœ¬åæ¨æè¿°ç”Ÿæˆå™¨
    åŠŸèƒ½ï¼šä½¿ç”¨GLM-4Væ¨¡å‹ç”Ÿæˆå›¾åƒæè¿°æˆ–çº¯æ–‡æœ¬æ¨ç†
    ä½¿ç”¨è¯´æ˜ï¼š**æ³¨æ„ï¼šåŸæ¨¡å‹åœ¨H20æ˜¾å¡ä¸Šæµ‹è¯•éœ€è‡³å°‘æœ‰24Gæ˜¾å­˜æ‰å¯è·‘**
    1. å°†æ¨¡å‹ä¸‹è½½æ”¾ç½®åœ¨models/LLMç›®å½•ä¸‹
    2. é€‰æ‹©æ¨¡å‹è·¯å¾„
    3. å¯é€‰æ‹©è¾“å…¥å›¾åƒæˆ–ä»…ä½¿ç”¨ç³»ç»Ÿè§’è‰²å’Œç”¨æˆ·è¾“å…¥
    4. è®¾ç½®ç”Ÿæˆå‚æ•°ï¼ˆæ¸©åº¦ã€top_pç­‰ï¼‰
    5. ç‚¹å‡»ç”Ÿæˆè·å–ç»“æœ
    """
    
    @classmethod
    def INPUT_TYPES(cls):
        # è·å–å½“å‰ComfyUIæ ¹ç›®å½•
        current_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
        models_path = os.path.join(current_dir, 'models/LLM')
        
        # æ‰«æ/models/LLMç›®å½•ä¸‹å¯ç”¨çš„æ¨¡å‹
        models = []
        if os.path.exists(models_path):
            models = [d for d in os.listdir(models_path) if os.path.isdir(os.path.join(models_path, d))]
        
        # option = True

        return {
            "required": {
                "æ¨¡å‹è·¯å¾„": (models, {"default": "GLM-4.1V-9B-Thinking"} if models else {}),
                "ç³»ç»Ÿè§’è‰²": ("STRING", {"multiline": True, "default": "You are a helpful assistant."}),
                "ç”¨æˆ·è¾“å…¥": ("STRING", {"multiline": True, "default": "ç®€å•æè¿°è¿™å¼ å›¾ç‰‡å†…å®¹"}),
                "æ¸©åº¦": ("FLOAT", {"default": 0.8, "min": 0.0, "max": 1.0, "step": 0.01}),
                "top_p": ("FLOAT", {"default": 0.6, "min": 0.0, "max": 1.0, "step": 0.01}),
                "top_k": ("INT", {"default": 2, "min": 1, "max": 200, "step": 1}),
                "æœ€å¤§æ–°tokenæ•°": ("INT", {"default": 8192, "min": 512, "max": 16384, "step": 1}),
                "é‡å¤æƒ©ç½š": ("FLOAT", {"default": 1.0, "min": 0.0, "max": 2.0, "step": 0.01}),
                "å¸è½½æ¨¡å‹": ("BOOLEAN", {"default": True}),
                "æ¨ç†æ¨¡å¼": (list(MyOptions._member_names_), {"default": "sdpa"}),
            },
            "optional": {
                "å›¾åƒ": ("IMAGE",),
            }
        }
    
    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("æè¿°",)
    FUNCTION = "describe_image"
    CATEGORY = "ğŸš¬é¦™çƒŸçš„å·¥å…·ç®±âœ…/3ï¸âƒ£æç¤ºè¯åæ¨ğŸ“"
    
    def __init__(self):
        self.model = None
        self.processor = None
    
    def describe_image(self, æ¨¡å‹è·¯å¾„, ç³»ç»Ÿè§’è‰², ç”¨æˆ·è¾“å…¥, æ¸©åº¦, top_p, top_k, æœ€å¤§æ–°tokenæ•°, é‡å¤æƒ©ç½š, å¸è½½æ¨¡å‹, æ¨ç†æ¨¡å¼, å›¾åƒ=None):
        # å¦‚æœæ¨¡å‹å·²åŠ è½½ä¸”éœ€è¦å¸è½½ï¼Œåˆ™å…ˆå¸è½½
        if å¸è½½æ¨¡å‹ and self.model is not None:
            del self.model
            del self.processor
            self.model = None
            self.processor = None
            torch.cuda.empty_cache()
        
        # æ„å»ºæ¶ˆæ¯
        messages = []
        
        # å¦‚æœæœ‰å›¾åƒè¾“å…¥ï¼Œåˆ™å¤„ç†å›¾åƒ
        if å›¾åƒ is not None:
            # å°†ComfyUIå›¾åƒå¼ é‡è½¬æ¢ä¸ºPILå›¾åƒ
            # å›¾åƒå¼ é‡æ ¼å¼ä¸º[N, H, W, C]ï¼Œå…¶ä¸­N=1, C=3 (RGB)
            image_np = å›¾åƒ[0].numpy() * 255.0
            image_np = np.clip(image_np, 0, 255).astype(np.uint8)
            pil_image = Image.fromarray(image_np)
            
            # ä¿å­˜å›¾åƒåˆ°ä¸´æ—¶æ–‡ä»¶
            temp_image_path = os.path.join(folder_paths.get_temp_directory(), "glm4v_temp_image.png")
            pil_image.save(temp_image_path)

            # print(f"ä¸´æ—¶å›¾ç‰‡è·¯å¾„: {temp_image_path}")
            
            messages.append(
                [
                    {
                        "role": "system",
                        "content": [
                            {
                                "type": "text",
                                "content": ç³»ç»Ÿè§’è‰²
                            }
                        ]
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "image", "url": temp_image_path},
                            {"type": "text", "text": ç”¨æˆ·è¾“å…¥}
                        ]
                    }
                ]
            )
        else:
            messages.append(
                [
                    {
                        "role": "system",
                        "content": [
                            {
                                "type": "text",
                                "content": ç³»ç»Ÿè§’è‰²
                            }
                        ]
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": ç”¨æˆ·è¾“å…¥}
                        ]
                    }
                ]
            )
        
        print(f"æ¨ç†æ¨¡å¼>>>>>>: {æ¨ç†æ¨¡å¼}")

        # å¦‚æœæ¨¡å‹æœªåŠ è½½ï¼Œåˆ™åŠ è½½æ¨¡å‹
        if self.model is None:
            full_model_path = os.path.join(models, æ¨¡å‹è·¯å¾„)
            self.processor = AutoProcessor.from_pretrained(full_model_path, use_fast=True)
            self.model = Glm4vForConditionalGeneration.from_pretrained(
                full_model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                attn_implementation= æ¨ç†æ¨¡å¼
            )
        
        # å¤„ç†è¾“å…¥
        inputs = self.processor.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_dict=True,
            return_tensors="pt",
            padding=True
        ).to(self.model.device)
                
        # ç”Ÿæˆè¾“å‡º
        output = self.model.generate(
            **inputs,
            max_new_tokens=æœ€å¤§æ–°tokenæ•°,
            repetition_penalty=é‡å¤æƒ©ç½š,
            do_sample=æ¸©åº¦ > 0,
            top_k=top_k,
            top_p=top_p,
            temperature=æ¸©åº¦ if æ¸©åº¦ > 0 else None,
        )
        
        # è§£ç è¾“å‡º
        raw = self.processor.decode(
            output[0][inputs["input_ids"].shape[1]:-1],
            skip_special_tokens=True
        )
        
        # å°è¯•è§£æJSONæ ¼å¼çš„è¾“å‡º
        json_data = None
        json_match = None
        if raw.startswith('{') or raw.startswith('['):
            json_data = json.loads(raw)
            json_match = re.search(r'<answer>(.*)', json.dumps(json_data), re.DOTALL)
        
        # åŒ¹é…ç­”æ¡ˆ
        original_match = re.search(r"<answer>(.*)", raw, re.DOTALL)
        match = json_match if json_match else original_match
        answer = match.group(1).strip() if match else raw.strip()
        
        print(f"[GLM4V] Prompt: {answer}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if å›¾åƒ is not None and os.path.exists(temp_image_path):
            os.remove(temp_image_path)
        
        # å¦‚æœéœ€è¦å¸è½½æ¨¡å‹ï¼Œåˆ™ç«‹å³å¸è½½
        if å¸è½½æ¨¡å‹:
            del self.model
            del self.processor
            self.model = None
            self.processor = None
            torch.cuda.empty_cache()
        
        return (answer,)


NODE_CLASS_MAPPINGS = {
    "GLM4VImageDescription_V2": GLM4VImageDescription_V2,
    "GLM4VTextToDescription": GLM4VTextToDescription
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "GLM4VImageDescription_V2": "ğŸš¬GLM-4Vå›¾åƒåæ¨æç¤ºè¯V2.0âœ…",
    "GLM4VTextToDescription": "ğŸš¬GLM-4Væ–‡æœ¬åæ¨æç¤ºè¯âœ…"
}